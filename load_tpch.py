import subprocess
import os
from pyspark.sql import SparkSession
import duckdb
from pyspark.sql.functions import lit

def log(message, level="INFO"):
    """
    Logs a message with a timestamp and log level.

    Args:
        message (str): The message to log.
        level (str, optional): The log level (e.g., "INFO", "WARN", "ERROR"). Defaults to "INFO".
    """
    from datetime import datetime
    now = datetime.now()
    timestamp = now.strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] [{level}] {message}")

def error_exit(message, exit_code=1):
    """
    Logs an error message and exits the script.

    Args:
        message (str): The error message.
        exit_code (int, optional): The exit code. Defaults to 1.
    """
    log(message, "ERROR")
    exit(exit_code)


def configure_environment(scale_factor, data_dir):
    """
    Configures the environment variables and creates the data directory.

    Args:
        scale_factor (int): The TPC-H scale factor.
        data_dir (str): The directory where TPC-H data will be stored.
    """
    log(f"Configuration: Scale Factor={scale_factor}, Data Dir={data_dir}")
    os.makedirs(data_dir, exist_ok=True)



def generate_tpch_data(scale_factor, data_dir):
    """
    Generates TPC-H data using DuckDB.

    Args:
        scale_factor (int): The TPC-H scale factor.
        data_dir (str): The directory where TPC-H data will be stored.
    """
    log(f"Generating TPC-H data with DuckDB (Scale Factor: {scale_factor})")
    try:
        duckdb.execute(f"INSTALL tpch; LOAD tpch; CALL dbgen(sf={scale_factor}); EXPORT DATABASE '{data_dir}' (FORMAT PARQUET);")
    except Exception as e:
        error_exit(f"DuckDB data generation failed: {e}")

    # Verify data generation
    parquet_files = [f for f in os.listdir(data_dir) if f.endswith(".parquet")]
    if not parquet_files:
        error_exit("No Parquet files generated by DuckDB")
    log(f"Generated {len(parquet_files)} Parquet files in {data_dir}")



def load_data_to_iceberg(spark, data_dir, iceberg_warehouse):
    """
    Loads the generated TPC-H data into Iceberg tables using Spark.

    Args:
        spark (SparkSession): The SparkSession.
        data_dir (str): The directory where TPC-H data is stored.
        iceberg_warehouse (str): The Iceberg warehouse location.
    """
    log("Loading data into Iceberg catalog")
    for filename in os.listdir(data_dir):
        if filename.endswith(".parquet"):
            table_name = filename.replace(".parquet", "")
            file_path = os.path.join(data_dir, filename)
            df = spark.read.parquet(file_path)
            # Use the Iceberg warehouse location
            full_table_name = f"demo.tpch.{table_name}"
            df.writeTo(full_table_name).using("iceberg").option("warehouse", iceberg_warehouse).createOrReplace()
            log(f"Created Iceberg table: {full_table_name} from {filename}")
    spark.sql("SHOW TABLES IN demo.tpch").show()



def main():
    """
    Main function to coordinate the TPC-H data generation and loading process.
    """
    scale_factor = int(os.environ.get("TPCH_SCALE_FACTOR", "1"))
    data_dir = os.environ.get("TPCH_DATA_DIR", "/home/iceberg/data/tpch_" + str(scale_factor))
    iceberg_warehouse = os.environ.get("ICEBERG_WAREHOUSE_PATH", "s3://warehouse")

    log("Starting TPC-H data generation and loading process")
    configure_environment(scale_factor, data_dir)
    generate_tpch_data(scale_factor, data_dir)

    # Initialize Spark session with Iceberg support.
    spark = SparkSession.builder \
        .appName("TPCH_Iceberg_Loader") \
        .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions") \
        .getOrCreate()

    load_data_to_iceberg(spark, data_dir, iceberg_warehouse)
    spark.stop()
    log("TPCH data generation and loading completed successfully", "SUCCESS")


if __name__ == "__main__":
    main()
